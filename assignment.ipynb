{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisites\n",
    "- Python have been installed\n",
    "- Jupyter environment set up in PyCharm or Visual Studio Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment\n",
    "Install required package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pip\n",
    "! pip install langchain langchain_community langchain_openai openai python-dotenv pypdf chromadb pysqlite3-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init variables\n",
    "You need to set value of `OPENAI_API_KEY` that you get from the training team in the `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_DOMAIN = os.getenv(\"AZURE_OPENAI_DOMAIN\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_API_ENDPOINT = f\"https://{AZURE_OPENAI_DOMAIN}.openai.azure.com\"\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_EMBEDDING_MODEL = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overviews\n",
    "The BonBon FAQ.pdf file contains frequently asked questions and answers for customer support scenario. The topics are around IT related issue troubleshooting such as networking, software, hardware. You are requested to provide a solution to build a chat bot capable of answering the user questions with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Document Indexing (mandatory)\n",
    "- The content of BonBon FAQ.pdf should be indexed to the local Chroma vector DB from where the chatbot can lookup the appropriate information to answer questions.\n",
    "- Should use some embedding model such as Azure Open AI text-embedding-ada-002 to create vectors, feel free to use any other open source embedding model if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path = \"./data/BonBon FAQ.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Initialize the RecursiveCharacterTextSplitter for splitting the documents into smaller chunks\n",
    "# - chunk_size: Maximum size of each text chunk (here, 1000 characters)\n",
    "# - chunk_overlap: Overlapping content between consecutive chunks (here, 200 characters)\n",
    "# This ensures that the content of the document is split while maintaining continuity across chunks.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split the loaded documents into smaller chunks using the text splitter\n",
    "# This method returns a list of chunks, where each chunk is a portion of the original document.\n",
    "text_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to fix an issue system has an unsupported version of sqlite3\n",
    "import pysqlite3\n",
    "import sys\n",
    "\n",
    "sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "# Initialize the AzureOpenAIEmbeddings with the appropriate configuration\n",
    "# - model: The deployment name of the Azure OpenAI embedding model.\n",
    "# - api_version: The version of the Azure OpenAI API you're using.\n",
    "# - api_key: Your Azure OpenAI API key for authentication.\n",
    "# - azure_endpoint: The Azure endpoint where the OpenAI service is deployed.\n",
    "embedding = AzureOpenAIEmbeddings(model=AZURE_OPENAI_EMBEDDING_MODEL,\n",
    "                                   api_version=AZURE_OPENAI_API_VERSION,\n",
    "                                   api_key=AZURE_OPENAI_API_KEY,\n",
    "                                   azure_endpoint=AZURE_OPENAI_API_ENDPOINT)\n",
    "\n",
    "# Create a Chroma vector store using the document chunks and embeddings\n",
    "# - documents: The list of text chunks generated from the PDF (previously split).\n",
    "# - embedding: The initialized AzureOpenAIEmbeddings object for converting documents into vector embeddings.\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=text_chunks,  # List of text chunks from the document\n",
    "    embedding=embedding     # AzureOpenAI embeddings to generate vector representations\n",
    ")\n",
    "\n",
    "# Convert the Chroma vector store into a retriever object\n",
    "# - The retriever enables similarity searches based on vector embeddings, useful for querying the document.\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Initialize the Azure OpenAI chat model with specific configurations\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=AZURE_OPENAI_DEPLOYMENT_NAME,  # The name of your Azure OpenAI deployment\n",
    "    api_version=AZURE_OPENAI_API_VERSION,           # Version of the Azure OpenAI API you are using\n",
    "    azure_endpoint=AZURE_OPENAI_API_ENDPOINT,       # The API endpoint to interact with Azure OpenAI\n",
    "    temperature=0,                                  # Temperature of 0 ensures deterministic output for factual QA tasks\n",
    "    max_tokens=None,                                # No strict limit on token generation (defaults may still apply)\n",
    "    timeout=None,                                   # No timeout explicitly set for API requests\n",
    "    max_retries=2,                                  # Allows for up to 2 retries in case of API call failures\n",
    "    streaming=False                                 # False indicates the response will not be streamed incrementally\n",
    ")\n",
    "\n",
    "# Define the prompt structure for the conversation\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # The system message defines the role of the model and context for the task\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an assistant for question-answering tasks.\"\n",
    "            \"Use the following pieces of IT related issue troubleshooting such as networking, software, hardware to answer the question.\"\n",
    "            \"If you don't know the answer, say that you don't know.\\n\\n{context}\"\n",
    "        ),\n",
    "        # The human message provides the user's question input\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a document-based question-answering chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Create a Retrieval-Augmented Generation (RAG) chain\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infinite loop for the chat interaction\n",
    "while True:\n",
    "    question = input(\"Human: \")\n",
    "    \n",
    "    if question == \"exit\":\n",
    "        break\n",
    "\n",
    "    result = rag_chain.invoke({\"input\": question})\n",
    "    \n",
    "    metadata = result['context'][0].metadata\n",
    "    filename = metadata['source'].split('/')[-1]\n",
    "    page = metadata['page']\n",
    "\n",
    "    print(f\"AI: {result['answer']}\\nSource: {filename} (page {page})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Building Chatbot (mandatory)\n",
    "- You are requested to build a chatbot solution for customer support scenario using Conversational ReAct agent supported in LangChain\n",
    "- The chatbot is able to support user to answer FAQs in the sample BonBon FAQ.pdf file.\n",
    "- The chatbot should use Azure Open AI GPT-3.5 LLM as the reasoning engine.\n",
    "- The chatbot should be context aware, meaning that it should be able to chat with users in the conversation manner.\n",
    "- The agent is equipped the following tools:\n",
    "  - Internet Search: Help the chatbot automatically find out more about something using Duck Duck Go internet search\n",
    "  - Knowledge Base Search: Help the chatbot to lookup information in the private knowledge base\n",
    "- In case user asks for information related to topics in the BonBon FAQ.pdf file such as internet connection, printer, malware issues the chatbot must use the private knowledge base, otherwise it should search on the internet to answer the question.\n",
    "- In the answer of chatbot, it should mention the source file and the page that the answer belongs to, for example the answer should mention \"BonBon FQA.pdf (page 2)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# Define the tools the agent can use\n",
    "\n",
    "# Create a retriever tool using a pre-existing retriever (e.g., a document retriever)\n",
    "tools = [\n",
    "    create_retriever_tool(\n",
    "        retriever=retriever,\n",
    "        name=\"HelpDesk\",\n",
    "        description=\"Use this tool to answer the related issue troubleshooting such as networking, software, hardware.\",\n",
    "    ),\n",
    "    \n",
    "    # Adding DuckDuckGo search tool for web search functionality\n",
    "    DuckDuckGoSearchRun(description=\"Use this tool to search information in the internet if don't have answer from another tools\"),\n",
    "]\n",
    "\n",
    "# Define a prompt template for the agent's conversation\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \n",
    "            \"You are an assistant for question-answering tasks.\"\n",
    "            \"You can answer question about IT related issues based on private documents.\"\n",
    "            \"If you can't get information from the private documents. Searching for the information from the internet instead.\"\n",
    "            \"If you don't know the answer, say that you don't know.\"\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a tool-calling agent using the defined LLM and the set of tools\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# Use ConversationBufferMemory to store and manage the chat history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Create an agent executor, which manages the interaction between the agent, tools, and memory\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    question = input(\"Human: \")\n",
    "    if question == \"exit\":\n",
    "        break\n",
    "        \n",
    "    result = agent_executor.invoke({\"input\": question})\n",
    "    print(f\"AI: {result['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Build a new assistant based on BonBon source code (optional)\n",
    "The objective\n",
    "- Run the code and index the sample BonBon FAQ.pdf file to Azure Cognitive Search\n",
    "- Explore the code and implement a new assistant that has the same behavior as above\n",
    "- Explore other features such as RBACs, features on admin portal\n",
    "\n",
    "Please contact the training team in case you need to get the source code of BonBon."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
